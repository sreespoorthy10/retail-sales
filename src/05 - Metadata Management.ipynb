{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04334e32-ddfb-4f3a-ac3b-439514f6456a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bd43dc-1dc4-4be9-82d7-c47708f0db79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Store metadata of data within tables\n",
    "This includes table verions, operations/tranmsformations performed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27991c54-0993-408f-8601-0ade9575936c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your catalog name here\n",
    "catalog_name = \"online_retail\"  # Change this to your actual catalog name\n",
    "\n",
    "metadata_df = None  # Initialize empty DataFrame for union\n",
    "\n",
    "# Get all schemas in the catalog\n",
    "schema_list = [row.databaseName for row in spark.sql(f\"SHOW SCHEMAS IN {catalog_name}\").collect() if row.databaseName != 'information_schema']\n",
    "\n",
    "# Loop through schemas\n",
    "for schema in schema_list:\n",
    "    # Get all tables in the schema\n",
    "    table_list = [row.tableName for row in spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema}\").collect() if row.tableName != '_sqldf' and \"metadata\" not in row.tableName]\n",
    "    \n",
    "    for table in table_list:\n",
    "        try:\n",
    "            # Run DESCRIBE HISTORY on each table\n",
    "            history_df = spark.sql(f\"DESCRIBE HISTORY {catalog_name}.{schema}.{table}\")\n",
    "            \n",
    "            # Add catalog, schema, and table name columns\n",
    "            history_df = history_df.withColumn(\"catalog_name\", lit(catalog_name)) \\\n",
    "                                   .withColumn(\"schema_name\", lit(schema)) \\\n",
    "                                   .withColumn(\"table_name\", lit(table))\n",
    "\n",
    "            # Reorder columns: catalog_name, schema_name, table_name first\n",
    "            column_order = [\"catalog_name\", \"schema_name\", \"table_name\"] + [col for col in history_df.columns if col not in [\"catalog_name\", \"schema_name\", \"table_name\"]]\n",
    "            history_df = history_df.select(*column_order)\n",
    "            \n",
    "            # Append to final DataFrame\n",
    "            metadata_df = history_df if metadata_df is None else metadata_df.union(history_df)\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Skipping {catalog_name}.{schema}.{table} due to error: {e}\")\n",
    "\n",
    "metadata_df.write.mode(\"overwrite\").saveAsTable(\"online_retail.silver.dataset_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "336ce7a0-2b29-40cb-9266-b8d42f329f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Store metadata of tables\n",
    "This gives details of tables like format, storage location, table size, partitioned columns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1801945e-8366-40ad-948d-946494d61d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your catalog name here\n",
    "catalog_name = \"online_retail\"  # Change this to your actual catalog name\n",
    "\n",
    "table_detail_metadata_df = None  # Initialize empty DataFrame for union\n",
    "\n",
    "# Get all schemas in the catalog\n",
    "schema_list = [row.databaseName for row in spark.sql(f\"SHOW SCHEMAS IN {catalog_name}\").collect() if row.databaseName != 'information_schema']\n",
    "\n",
    "# Loop through schemas\n",
    "for schema in schema_list:\n",
    "    # Get all tables in the schema\n",
    "    table_list = [row.tableName for row in spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema}\").collect() if row.tableName != '_sqldf' and \"metadata\" not in row.tableName]\n",
    "    \n",
    "    for table in table_list:\n",
    "        try:\n",
    "            # Run DESCRIBE HISTORY on each table\n",
    "            detail_df = spark.sql(f\"DESCRIBE DETAIL {catalog_name}.{schema}.{table}\")\n",
    "            \n",
    "            # Add catalog, schema, and table name columns\n",
    "            detail_df = detail_df.withColumn(\"catalog_name\", lit(catalog_name)) \\\n",
    "                                   .withColumn(\"schema_name\", lit(schema)) \\\n",
    "                                   .withColumn(\"table_name\", lit(table))\n",
    "\n",
    "            # Reorder columns: catalog_name, schema_name, table_name first\n",
    "            column_order = [\"catalog_name\", \"schema_name\", \"table_name\"] + [col for col in detail_df.columns if col not in [\"catalog_name\", \"schema_name\", \"table_name\"]]\n",
    "            detail_df = detail_df.select(*column_order)\n",
    "            \n",
    "            # Append to final DataFrame\n",
    "            table_detail_metadata_df = detail_df if table_detail_metadata_df is None else table_detail_metadata_df.union(detail_df)\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Skipping {catalog_name}.{schema}.{table} due to error: {e}\")\n",
    "\n",
    "\n",
    "table_detail_metadata_df.write.mode(\"overwrite\").saveAsTable(\"online_retail.silver.table_detail_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300288b3-adde-4255-a3fc-447425d45bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Store metadata of columns within tables\n",
    "This gives datatypes of columns within each table, comments, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d11c4be-05fd-4ade-844b-262ed1a5a98b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your catalog name here\n",
    "catalog_name = \"online_retail\"  # Change this to your actual catalog name\n",
    "\n",
    "table_extended_metadata_df = None  # Initialize empty DataFrame for union\n",
    "\n",
    "# Get all schemas in the catalog\n",
    "schema_list = [row.databaseName for row in spark.sql(f\"SHOW SCHEMAS IN {catalog_name}\").collect() if row.databaseName != 'information_schema']\n",
    "\n",
    "# Loop through schemas\n",
    "for schema in schema_list:\n",
    "    # Get all tables in the schema\n",
    "    table_list = [row.tableName for row in spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema}\").collect() if row.tableName != '_sqldf' and \"metadata\" not in row.tableName]\n",
    "    \n",
    "    for table in table_list:\n",
    "        try:\n",
    "            # Run DESCRIBE HISTORY on each table\n",
    "            extended_df = spark.sql(f\"DESCRIBE EXTENDED {catalog_name}.{schema}.{table}\")\n",
    "            \n",
    "            # Add catalog, schema, and table name columns\n",
    "            extended_df = extended_df.withColumn(\"catalog_name\", lit(catalog_name)) \\\n",
    "                                   .withColumn(\"schema_name\", lit(schema)) \\\n",
    "                                   .withColumn(\"table_name\", lit(table))\n",
    "\n",
    "            # Reorder columns: catalog_name, schema_name, table_name first\n",
    "            column_order = [\"catalog_name\", \"schema_name\", \"table_name\"] + [col for col in extended_df.columns if col not in [\"catalog_name\", \"schema_name\", \"table_name\"]]\n",
    "            extended_df = extended_df.select(*column_order)\n",
    "            \n",
    "            # Append to final DataFrame\n",
    "            table_extended_metadata_df = extended_df if table_extended_metadata_df is None else table_extended_metadata_df.union(extended_df)\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Skipping {catalog_name}.{schema}.{table} due to error: {e}\")\n",
    "\n",
    "\n",
    "table_extended_metadata_df.write.mode(\"overwrite\").saveAsTable(\"online_retail.silver.table_extended_metadata\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6573919877241384,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - Metadata Management",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
